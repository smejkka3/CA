{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cognitive Algorithms - Assignment 5 (30 points)\n",
    "Cognitive Algorithms        \n",
    "Summer term 2018      \n",
    "Technische Universit√§t Berlin     \n",
    "Fachgebiet Maschinelles Lernen \n",
    "\n",
    "**Due on July 4, 2018 10am via ISIS  **\n",
    "                    \n",
    "After completing all tasks, run the whole notebook so that the content of each cell is properly displayed. Make sure that the code was ran and the entire output (e.g. figures) is printed. Print the notebook as a PDF file and again make sure that all lines are readable - use line breaks in the Python Code '\\' if necessary. Points will be deducted, if code or content is not readable!                  \n",
    "           \n",
    "**Upload the PDF file that contains a copy of your notebook on ISIS.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Multiple Choice Questions (5 points)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A)** Which statements about unsupervised learning are true?\n",
    "- [ ] Its goal is to learn a mapping from input data to output data\n",
    "- [ ] Its goal is to find structure in the data\n",
    "- [ ] It needs labels for training \n",
    "- [ ] It does not need labels for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B)** Which of the following methods solve a supervised learning problem?\n",
    "- [ ] Linear Discriminant Analysis\n",
    "- [ ] Nearest Centroid Classifier\n",
    "- [ ] Non-negative Matrix Factorization\n",
    "- [ ] K-Means Clustering\n",
    "- [ ] Perceptron\n",
    "- [ ] Ordinary Least Squares\n",
    "- [ ] (Kernel) Ridge Regression\n",
    "- [ ] Principal Component Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C)** Which statement about the Principal Components Analysis is **not** true?\n",
    "- [ ] PCA finds the direction that maximizes the variance of the projected data\n",
    "- [ ] The PCs are uncorrelated\n",
    "- [ ] The first $k$ PCs are the eigenvectors corresponding to the smallest $k$ eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D)** Cross-Validation can be used to ...\n",
    "- [ ] ... estimate the generalization error\n",
    "- [ ] ... find optimal parameter values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E)** Nested Cross-Validation can be used to ...\n",
    "- [ ] ... estimate the generalization error\n",
    "- [ ] ... find optimal parameter values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Programming (25 points)\n",
    "---\n",
    "### Task 1: Principal Component Analysis (16 points)\n",
    "In this assignment, you will detect trends in text data and implement Principal Component Analysis (PCA). The text data consists of preprocessed news feeds gathered from http://beta.wunderfacts.com/ in October 2011, and you will be able to detect  a trend related to Steve Jobs death on 5th October 2011. \n",
    "\n",
    "The data consists of 26800 Bag-of-Words (BOW) features of news published every hour, i.e. the news are represented in a vector which contains the occurence of each word. Here we have many more dimensions (26800) than data points (645). This is why we will implement Linear Kernel PCA instead of standard PCA. \n",
    "\n",
    "Download the data set ```newsdata.npz```, if not done yet.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as pl\n",
    "import scipy as sp\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pca(X,ncomp=10):\n",
    "    ''' Principal Component Analysis\n",
    "    INPUT:  X       - DxN array of N data points with D features\n",
    "            ncomp   - number of principal components to estimate \n",
    "    OUTPUT: W       - D x ncomp array of directions of maximal variance, \n",
    "                     sorted by their eigenvalues\n",
    "            H       - ncomp x N array of projected data ''' \n",
    "    ncomp = min(np.hstack((X.shape, ncomp)))\n",
    "    #center the data\n",
    "    # ... \n",
    "    \n",
    "    # compute linear kernel\n",
    "    #...\n",
    "    \n",
    "    # compute eigenvectors and sort them according to their eigenvalues\n",
    "    # ...\n",
    "    \n",
    "    # compute W and H \n",
    "    #...\n",
    "\n",
    "def get_data(fname='newsdata_BOW.npz'):\n",
    "    foo = np.load(fname)\n",
    "    dates = foo['dates']\n",
    "    BOW = np.array(foo['BOW_features'].tolist().todense())\n",
    "    words = foo['words']\n",
    "    return BOW,words,dates\n",
    "    \n",
    "def nmf(X,ncomp=10,its=100):\n",
    "    '''Non-negative matrix factorization as in Lee and Seung http://dx.doi.org/10.1038/44565\n",
    "    INPUT:  X       -  DxN array of N data points with D features\n",
    "            ncomp   - number of factors to estimate\n",
    "            its     - number of iterations\n",
    "    OUTPUT: W       - D x ncomp array\n",
    "            H       - ncomp x N array ''' \n",
    "    ncomp = min(np.hstack((X.shape, 10)))\n",
    "    X = X + 1e-19\n",
    "    # initialize randomly\n",
    "    W = sp.random.rand(X.shape[0],ncomp)\n",
    "    H = sp.random.rand(X.shape[1],ncomp).T\n",
    "    # update for its iterations\n",
    "    for it in sp.arange(its):\n",
    "        H = H * (W.T.dot(X)/(W.T.dot(W.dot(H))))\n",
    "        W = W * (X.dot(H.T)/(W.dot(H.dot(H.T))))\n",
    "    return W,H\n",
    "    \n",
    "def plot_trends(ntopics=8,method=nmf,topwhat=10):\n",
    "    #load data\n",
    "    BOW,words,dates = get_data()\n",
    "    topics,trends = method(BOW,ntopics)\n",
    "    for itopic in range(ntopics):\n",
    "        pl.figure(figsize=(8,6))\n",
    "        pl.plot(trends[itopic,:].T)\n",
    "        ranks = (-abs(topics[:,itopic])).argsort()\n",
    "        thislabel = words[ranks[:topwhat]]\n",
    "        pl.legend([thislabel])\n",
    "        days = sp.arange(0,BOW.shape[-1],24*7)\n",
    "        pl.xticks(days,dates[days],rotation=20)\n",
    "\n",
    "def test_assignment6():\n",
    "    ##Example 1\n",
    "    X = sp.array([[0, 1], [0, 1]])\n",
    "    W, H = pca(X, ncomp = 1)\n",
    "    assert(sp.all(W / W[0] == [[1], [1]])) \n",
    "    print '2 datapoint test passed'\n",
    "    \n",
    "    ##Example 2\n",
    "    #generate 2D data\n",
    "    N =100\n",
    "    cov = sp.array([[10, 4], [4, 5]])\n",
    "    X = sp.random.multivariate_normal([0, -20], cov, N).T\n",
    "    #do pca \n",
    "    W, H = pca(X)\n",
    "    #plot result\n",
    "    pl.figure()\n",
    "    pc0 = 10*W[:,0] / np.linalg.norm(W[:,0])\n",
    "    pc1 = 10*W[:,1] / np.linalg.norm(W[:,1])\n",
    "    pl.plot([-pc0[0], pc0[0]], [-pc0[1]-20, pc0[1]-20], '-k', label='1st PC')\n",
    "    pl.hold(True)\n",
    "    pl.plot([-pc1[0], pc1[0]], [-pc1[1]-20, pc1[1]-20], '-.r', label='2nd PC')\n",
    "    pl.plot(X[0,:], X[1,:], '+', color='k')\n",
    "    pl.axis('equal')\n",
    "    pl.legend(loc=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A) (7 points)** Implement Linear Kernel Principal Component Analysis  by completing the function stub  ```pca```.  Given data $X \\in \\mathbb{R}^{D \\times N}$, PCA finds a decomposition of the data in $k$ orthogonal principal components that maximize the variance in the data, \n",
    "\n",
    "$$X = W \\cdot H$$ \n",
    "\n",
    "with $W \\in \\mathbb{R}^{D \\times k}$ and   $H \\in \\mathbb{R}^{k \\times N}$. The  Pseudocode is given below. The function  ```test_assignment6``` helps you to debug your code. It plots for a 2D data set the two principal components. \n",
    "\n",
    "```PCA( X, k ):```\n",
    "1. ```# Require: data``` $x_1, \\ldots, x_N \\in \\mathbb R^d$, $N\\ll d$ ```, number of principal components``` $k$\n",
    "1. ```# Center Data```\n",
    "1. $\\; \\; \\; X = X - 1/N\\sum_ix_i$\n",
    "1. ```# Compute Linear Kernel```\n",
    "1. $\\; \\; \\; K = X^{\\top}X$\n",
    "1. ```# Compute eigenvectors corresponding to the``` $k$ ```largest eigenvalues```\n",
    "1. $\\; \\; \\; \\alpha = \\text{eig}(K)$\n",
    "1. $\\; \\; \\;  W = X \\alpha$\n",
    "1. ```# Project data onto``` $W$\n",
    "1. $\\; \\; \\; H = W^\\top X$\n",
    "1. ```return W, H```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_assignment6()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B) (3 points)** What happens when you forget to center the data in ```pca```? Show the resulting plot for the 2D toydata example and explain the result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Your answer for B) here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_assignment6()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C) (3 points)** Suppose we only have two data points, $\\begin{bmatrix}0 \\\\ 1 \\end{bmatrix}$ and $\\begin{bmatrix}0 \\\\ 2 \\end{bmatrix}$, $X = \\begin{bmatrix} 0 & 0 \\\\ 1 & 2 \\end{bmatrix}$. What would be the principal directions $W = [\\mathbf w_1, \\mathbf w_2]$? What will be the variance of the projected data onto each of the principal components $\\mathbb V(\\mathbf w_1^T X)$, $\\mathbb V(\\mathbf w_2^T X)$? What is $H$? \n",
    "\n",
    "*Hint:* You can obtain the result simply by visualizing the two data points and remembering PCA's objective. Or you can calculate the result using ```standard``` PCA. With Linear Kernel PCA, you will not be able to compute $\\mathbf{w_2}$, because the corresponding eigenvalue is $0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Your answer for C) here]**          \n",
    "$\\mathbf{w}_1 = \\\\\n",
    "\\mathbf{w}_2 = \\\\\n",
    "\\mathbb V(\\mathbf w_1^T X) = \\\\\n",
    "\\mathbb V(\\mathbf w_2^T X) = \\\\\n",
    "H =$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D) (3 points)** Detect trends in the text data by calling the provided function ```plot_trends``` once for PCA and once for Non-Negative Matrix Factorization (NMF) (the code for NMF is provided as well). Which differences do you notice between the algorithms? Which method would you prefer for this task? Hand in the plot of the most prominent trend related to Steve Jobs death for each algorithm.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Your answer for D) here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_trends(method=pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_trends(method=nmf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: K-Means Clustering (9 points)\n",
    "In this exercise we want to implement the $K$-Means Clustering algorithm. It finds cluster centers $\\mu_1 \\ldots \\mu_K$ such that the distance of the data points to their respective cluster center are minimized. This is done by re-iterating two steps:\n",
    "1. Assign each data point $x_n$ to their closest cluster $\\mu_k$ (for all $n = 1 \\ldots N$)\n",
    "1. Update each cluster center $\\mu_k$ to the mean of the members in that cluster $k$ (for all $k = 1 \\ldots K$)\n",
    "\n",
    "Complete the function ```kmeans``` (see Task 2.A to 2.D for more detail). ```test_kmeans``` helps you to debug your code. It generates a simple 2D toy dataset. Your ```kmeans``` implementation should correctly identify the three clusters and should converge after only a few iterations (less than 10)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A) (2 points)** Initialize the centroids. To do so calculate the mean of the whole data set and add some standard normal distributed noise to it, i.e. for all $k = 1 \\ldots K$\n",
    "\n",
    "$$\\mu_k = \\bar{x} + \\epsilon_k$$\n",
    "\n",
    "where $\\bar{x}, \\epsilon_k \\in \\mathbb{R}^D$ and $\\bar{x} = \\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{x_n}$ and $\\epsilon_k \\sim \\mathcal{N}(\\mathbf{0}, I)$ \n",
    "\n",
    "**B) (4 points)** For step 1 of the algorithm, we need the distance between each data point $x_n$ and each centroid $\\mu_k$. Complete the function ```distmat``` that calculates a matrix $Dist \\in \\mathbb{R}^{N,K}$ such that \n",
    "\n",
    "$$Dist_{n,k} = || x_n - \\mu_k ||_2^2 $$\n",
    "\n",
    "We can calculate the matrix $Dist$ without the use of for-loops by using following formula:\n",
    "\n",
    "$$Dist = A - 2 B + C  $$\n",
    "\n",
    "where $A_{n,k} = x_n^T x_n \\; \\; , \\; \\; B_{n,k} = x_n^T \\mu_k \\;$ and $\\; C_{n,k} = \\mu_k^T \\mu_k$\n",
    "\n",
    "**C) (1 point)** Assign each data point to its closest centroid. To do so, construct a matrix $Closest \\in \\mathbb{R}^{N,K}$ such that \n",
    "\n",
    "$$Closest_{n,k} = \\begin{cases} 1 & \\; \\; \\tt{if} \\; \\; \\mu_k \\text{ is the closest centroid to } x_k \\\\ 0 & \\; \\; \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "i.e. each row of $Closest$ holds only one non-zero element.\n",
    "\n",
    "**D) (2 points)** Update each cluster center to the mean of the members in that cluster, i.e. for all $k = 1 \\ldots K$ \n",
    "\n",
    "$$\\mu_k = \\frac{1}{|{\\mathcal{X_k}}|} \\sum_{x \\in \\mathcal{X_k}} x$$ \n",
    "\n",
    "$$\\mathcal{X_k} = \\{ x_n \\in X \\; | \\text{ the closest centroid to } x_n \\text{ is } \\mu_k \\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeans(X, K, max_iter=50, eta=0.05):\n",
    "    \"\"\" k-Means Clustering\n",
    "    INPUT:  X            - DxN array of N data points with D features\n",
    "            K            - number of clusters\n",
    "            max_iter     - maximum number of iterations\n",
    "            eta          - small threshold for convergence criterion\n",
    "    OUTPUT: centroids    - DxK array of K centroids with D features\n",
    "            closest      - NxK array that indicates for each of the N data points\n",
    "                           in X the closest centroid after convergence.\n",
    "                           Each row in closest only holds one non-zero entry. \n",
    "                           closest[n,k] == 1 <=> \n",
    "                           centroids[:,k] is closest to data point X[:,n]\n",
    "    \"\"\"\n",
    "    D,N = np.shape(X)\n",
    "    dist = np.zeros([N,K])\n",
    "    closest = np.zeros([N,K])\n",
    "    # initialize the centroids (close to the mean of X)\n",
    "    # ... your code here ...\n",
    "    \n",
    "    cur_iter = 0\n",
    "    while cur_iter < max_iter:\n",
    "        plot_cluster(X, centroids, closest)\n",
    "        cur_iter += 1\n",
    "        old_centroids = centroids.copy()\n",
    "        # calculate the distance between each data point and each centroid\n",
    "        dist = distmat(X,centroids)\n",
    "        # get for each data point in X it's closest centroid\n",
    "        # ... your code here ...\n",
    "        \n",
    "        # update the estimation of the centroids\n",
    "        # ... your code here ...\n",
    "        \n",
    "        if np.linalg.norm(old_centroids - centroids) < eta:\n",
    "            print 'Converged after ' + str(cur_iter) + ' iterations.'\n",
    "            break;\n",
    "    return centroids, closest\n",
    "\n",
    "def distmat(X, Y):\n",
    "    \"\"\" Distance Matrix\n",
    "    INPUT:      X           - DxN array of N data points with D features\n",
    "                Y           - DxM array of M data points with D features\n",
    "    OUTPUT:     distmat     - NxM array s.t. D[n,m] = || x_n - y_m ||^2\n",
    "    Hint: np.tile might be helpful\n",
    "    \"\"\"\n",
    "    D_x,N = np.shape(X)\n",
    "    D_y,M = np.shape(Y)\n",
    "    assert D_x == D_y\n",
    "    # calculate the distance matrix\n",
    "    # ... your code here ...\n",
    "    \n",
    "    \n",
    "def test_kmeans():\n",
    "    #generate 2D data\n",
    "    N =500\n",
    "    cov = np.array([[1, 0], [0, 0.5]])\n",
    "    # generate for each of the three clusters N data points\n",
    "    x1 = np.random.multivariate_normal([-2, 2], cov, N) \n",
    "    x2 = np.random.multivariate_normal([2, 2], cov, N) \n",
    "    x3 = np.random.multivariate_normal([-2, -2], cov, N)\n",
    "    X = np.vstack((x1, x2, x3)).transpose()\n",
    "    \n",
    "    # run kmeans and plot the result\n",
    "    centroids, closest = kmeans(X, 3)\n",
    "    plot_cluster(X, centroids, closest)\n",
    "\n",
    "    \n",
    "def plot_cluster(X, centroids, closest):\n",
    "    K = np.shape(centroids)[1]\n",
    "    plt.figure()\n",
    "    plt.scatter(X[0], X[1])\n",
    "    if (closest != np.zeros(np.shape(closest))).any():\n",
    "        for k in xrange(K):\n",
    "            # get for each centroid the assigned data points\n",
    "            Xk = X[:, closest[:,k]]\n",
    "            # plot each cluster in a different color\n",
    "            plt.scatter(Xk[0], Xk[1])\n",
    "    # plot each centroid (should be center of cloud)\n",
    "    plt.scatter(centroids[0], centroids[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_kmeans()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
