{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cognitive Algorithms - Assignment 4 (30 points)\n",
    "Cognitive Algorithms        \n",
    "Summer term 2018      \n",
    "Technische Universit√§t Berlin     \n",
    "Fachgebiet Maschinelles Lernen \n",
    "\n",
    "**Due on June 13, 2018 10am via ISIS **\n",
    "                    \n",
    "After completing all tasks, run the whole notebook so that the content of each cell is properly displayed. Make sure that the code was ran and the entire output (e.g. figures) is printed. Print the notebook as a PDF file and again make sure that all lines are readable - use line breaks in the Python Code '\\' if necessary. Points will be deducted, if code or content is not readable!                  \n",
    "           \n",
    "**Upload the PDF file that contains a copy of your notebook on ISIS.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group:        \n",
    "Members:          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Theory (3 points)\n",
    "**A) (2 points)** Explain briefly the goal of classification and regression. What is the difference between both tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Your answer for A) here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B)** Which statement is true?\n",
    "- [ ] Classification is a supervised learning task, regression is an unsupervised learning task.       \n",
    "- [ ] Classification is an unsupervised learning task, regression is a supervised learning task.      \n",
    "- [ ] Classification and regression are both supervised learning tasks.                 \n",
    "- [ ] Classification and regression are both unsupervised learning tasks.               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Programming (27 points)\n",
    "---\n",
    "Note that part 2 of this assignment consists of two tasks.\n",
    "\n",
    "### Task 1: Ordinary Least Squares (9 points)\n",
    "In this assignment you will implement a linear regression and predict two dimensional hand positions from electromyographic (EMG) recordings obtained with high-density electrode arrays on the lower arm.  Download the data set ```myo_data.mat``` from the ISIS web site, if not done yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "import scipy as sp\n",
    "from numpy.linalg import inv\n",
    "from scipy.io import loadmat\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_myo_data(fname):\n",
    "    ''' Loads EMG data from <fname>                      \n",
    "    '''\n",
    "    # load the data\n",
    "    data = loadmat(fname)\n",
    "    # extract data and hand positions\n",
    "    X = data['training_data']\n",
    "    X = sp.log(X)\n",
    "    Y = data['training_labels']\n",
    "    #Split data into training and test data\n",
    "    X_train = X[:, :5000]\n",
    "    X_test = X[:, 5000:]\n",
    "    Y_train = Y[:, :5000]\n",
    "    Y_test = Y[:, 5000:]\n",
    "    return X_train,Y_train,X_test, Y_test\n",
    "\n",
    "def train_ols(X_train, Y_train, llambda = 0):\n",
    "    ''' Trains ordinary least squares (ols) regression \n",
    "    Input:       X_train  -  DxN array of N data points with D features\n",
    "                 Y        -  D2xN array of length N with D2 multiple labels\n",
    "                 llabmda  -  Regularization parameter\n",
    "    Output:      W        -  DxD2 array, linear mapping used to estimate labels \n",
    "                             with sp.dot(W.T, X)                      \n",
    "    '''\n",
    "    #your code here\n",
    "    \n",
    "def apply_ols(W, X_test):\n",
    "    ''' Applys ordinary least squares (ols) regression \n",
    "    Input:       X_test    -  DxN array of N data points with D features\n",
    "                 W        -  DxD2 array, linear mapping used to estimate labels \n",
    "                             trained with train_ols                   \n",
    "    Output:     Y_test    -  D2xN array\n",
    "    '''\n",
    "    #your code here\n",
    "    \n",
    "def predict_handposition():\n",
    "    X_train,Y_train,X_test, Y_test = load_myo_data('myo_data.mat')\n",
    "    # compute weight vector with linear regression\n",
    "    W = train_ols(X_train, Y_train)\n",
    "    # predict hand positions\n",
    "    Y_hat_train = apply_ols(W, X_train)\n",
    "    Y_hat_test = apply_ols(W, X_test) \n",
    "        \n",
    "    pl.figure(figsize=(8,6))\n",
    "    pl.subplot(2,2,1)\n",
    "    pl.plot(Y_train[0,:1000],Y_train[1,:1000],'.k',label = 'true')\n",
    "    pl.plot(Y_hat_train[0,:1000],Y_hat_train[1,:1000],'.r', label = 'predicted')\n",
    "    pl.title('Training Data')\n",
    "    pl.xlabel('x position')\n",
    "    pl.ylabel('y position')\n",
    "    pl.legend(loc = 0)\n",
    "    \n",
    "    pl.subplot(2,2,2)\n",
    "    pl.plot(Y_test[0,:1000],Y_test[1,:1000],'.k')\n",
    "    pl.plot(Y_hat_test[0,:1000],Y_hat_test[1,:1000],'.r')\n",
    "    pl.title('Test Data')\n",
    "    pl.xlabel('x position')\n",
    "    pl.ylabel('y position')\n",
    "    \n",
    "    pl.subplot(2,2,3)\n",
    "    pl.plot(Y_train[1,:600], 'k', label = 'true')\n",
    "    pl.plot(Y_hat_train[1,:600], 'r--', label = 'predicted')\n",
    "    pl.xlabel('Time')\n",
    "    pl.ylabel('y position')\n",
    "    pl.legend(loc = 0)\n",
    "    \n",
    "    pl.subplot(2,2,4)\n",
    "    pl.plot(Y_test[1,:600],'k')\n",
    "    pl.plot(Y_hat_test[1,:600], 'r--')\n",
    "    pl.xlabel('Time')\n",
    "    pl.ylabel('y position')\n",
    "    \n",
    "def test_assignment4():\n",
    "    ##Example without noise\n",
    "    x_train = sp.array([[ 0,  0,  1 , 1],[ 0,  1,  0, 1]])\n",
    "    y_train = sp.array([[0, 1, 1, 2]])\n",
    "    w_est = train_ols(x_train, y_train) \n",
    "    w_est_ridge = train_ols(x_train, y_train, llambda = 1)\n",
    "    assert(sp.all(w_est.T == [[1, 1]])) \n",
    "    assert(sp.all(w_est_ridge.T == [[.75, .75]]))\n",
    "    y_est = apply_ols(w_est,x_train)\n",
    "    assert(sp.all(y_train == y_est)) \n",
    "    print 'No-noise-case tests passed'\n",
    "    \n",
    "    ##Example with noise\n",
    "    #Data generation\n",
    "    w_true = 4\n",
    "    X_train = sp.arange(10)\n",
    "    X_train = X_train[None,:]\n",
    "    Y_train = w_true * X_train + sp.random.normal(0,2,X_train.shape)\n",
    "    #Regression \n",
    "    w_est = train_ols(X_train, Y_train) \n",
    "    Y_est = apply_ols(w_est,X_train)\n",
    "    #Plot result\n",
    "    pl.figure()\n",
    "    pl.plot(X_train.T, Y_train.T, '+', label = 'Train Data')\n",
    "    pl.plot(X_train.T, Y_est.T, label = 'Estimated regression')\n",
    "    pl.xlabel('x')\n",
    "    pl.ylabel('y')\n",
    "    pl.legend(loc = 'lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** A) (5 points)** Implement ordinary least squares regression (OLS) with an optional ridge parameter by completing the function stubs  ```ols_train``` and  ```ols_apply```. In ```ols_train```, you estimate a linear mapping $W$,    \n",
    "$$W = (X_{\\text{train}}X_{\\text{train}}^{\\top} + \\lambda I)^{-1}X_{\\text{train}}Y_{\\text{train}}^{\\top}$$       \n",
    "that optimally predicts the training labels from the training data, $X_{\\text{train}} \\in \\mathbb{R}^{D_X \\times N_{tr}}$,  $Y_{\\text{train}} \\in\\mathbb{R}^{D_Y \\times N_{tr}}$. Here, $\\lambda \\in \\mathbb R$ is the (optional) Ridge regularization parameter.  \n",
    "The function ```ols_apply``` than uses the weight vector to predict the (unknown) hand positions of new test data $X_{\\text{test}} \\in\\mathbb{R}^{D_X \\times N_{te}}$     \n",
    "$$Y_{\\text{test}} = W^{\\top}X_{\\text{test}}$$         \n",
    "The function  ```test_assignment4``` helps you to debug your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_assignment4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B) (1 point)**  The data set ```myo_data.mat``` consists of preprocessed EMG data $X$ and 2-dimensional stimulus labels $Y$. Labels are x/y positions of the hand during different hand movements. The function  ```load_myo_data```  loads the data and splits it into train and test data. Familiarize yourself with the data by answering the following questions:         \n",
    "How many time points $N_{tr}$ does the train set contain? How many time points $N_{te}$ does the test set contain? At each time point, at how many electrodes $D_X$ was the EMG collected? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$N_{tr} =$ **[Your answer here]**                   \n",
    "$N_{te} =$ **[Your answer here]**                        \n",
    "$D_{X} =$ **[Your answer here]**                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C) (1 points)** Predict two dimensional hand positions by calling the function ```predict_handpositions```. It plots, for the train and the test data, the true hand position versus the estimated hand position. Do you notice a performance difference between train and test data set? Is this a surprising result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Your answer for C) here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_handposition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D) (1 points)** In the previous tasks, we have used the logarithmized muscle activiations to predict the hand positions. Comment the line where we logarithmize the EMG features in the function  ```load_myo_data``` and call  ```predict_handpositions``` again. Do you notice a performance difference compared to the logarithmized version? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Your answer for D) here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_handposition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E) (1 points)** If we cannot predict the labels $Y$ perfectly by a linear regression on $X$, does this imply that the relationship between $X$ and $Y$ is non-linear? Explain your decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Your answer for E) here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Polynomial Regression (18 points)\n",
    "In task 1 you implemented linear regression. However, you will see in this task, that aboves code can be generalized to polynomial regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A) (9 points)**  Write a function  ```test_polynomial_regression``` which generates toy data and visualizes the results from a polynomial regression. The goal is to create two plots as in the Figure below (Note that your figure will look slightly different, because the data is generated randomly.)     \n",
    "To do so, first create toy data from a sine function as follows:           \n",
    "$$x_i \\in \\{0, 1, 2, \\ldots, 10\\}, y_i = \\sin(x_i) + \\epsilon_i, \\; \\; \\epsilon_i \\sim \\mathcal{N}(0, 0.5)$$        \n",
    "where $\\mathcal{N}$(mean, standard deviation) denotes the Gaussian distribution and $i \\in \\{1, 2, \\ldots, 11\\}$ is an index. Then implement polynomial regression, which models the relationship between $y$ and $x$ as an $m$th order polynomial, i.e. \n",
    "$\\hat{y} = w_0 + w_1 x + w_2 x^2 + \\ldots + w_m x^m$. The parameters $w_0, w_1, \\ldots , w_m \\in \\mathbb R$ are estimated by Ridge Regression. \n",
    "\n",
    "*Hint:* You can use your functions ```ols_train``` and  ```ols_apply```, if you build an appropriate data matrix (for loops are allowed to do so).\n",
    "\n",
    "Apply and visualize polynomial ridge regression for different parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure_1](Figure_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_polynomial_regression()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B) (9 points)** Run your code of task A) multiple times.\n",
    "- (4 points) What do you observe for the different values of the parameters $m$ and $\\lambda$? Explain this behaviour. \n",
    "- (2 points) Decide for each of the two figures which values of the parameters yield the best fit. \n",
    "- (3 points) Do you expect those parameters to perform good on all possible data sets? Explain your decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Your answers for B) here]**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
