{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cognitive Algorithms - Assignment 6 (30 points)\n",
    "Cognitive Algorithms        \n",
    "Summer term 2018      \n",
    "Technische Universität Berlin     \n",
    "Fachgebiet Maschinelles Lernen \n",
    "\n",
    "**Due on July 16, 2018 10am via ISIS **\n",
    "                    \n",
    "After completing all tasks, run the whole notebook so that the content of each cell is properly displayed. Make sure that the code was ran and the entire output (e.g. figures) is printed. Print the notebook as a PDF file and again make sure that all lines are readable - use line breaks in the Python Code '\\' if necessary. Points will be deducted, if code or content is not readable!                  \n",
    "           \n",
    "**Upload the PDF file that contains a copy of your notebook on ISIS.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group:     \n",
    "Members:     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Theory (15 points)\n",
    "---\n",
    "### Task 1: Multiple Choice Questions (2 points)\n",
    "**A)** A Multilayer Perceptron can be used for ...           \n",
    "- [x] classification                   \n",
    "- [x] regression                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B)** The training algorithm of the MLP mainly consists of two phases. Which statement about the backward phase is true?       \n",
    "- [x] the error of each neuron is computed for each neuron, starting with the neurons in the output layer          \n",
    "- [ ] the error of each neuron is computed for each neuron, starting with the neurons in the input layer               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Learning Procedure (5 points)\n",
    "Before we can use an MLP for a given task, we have to train it. This training procedure (here: batch mode) is composed of different steps, that you can find below. However, the order of the steps is not correct. Please bring the steps in the correct order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. FOR EACH input vector\n",
    "1. END FOR EACH\n",
    "1. REPEAT until stopping criterion is fulfilled\n",
    "1. END REPEAT\n",
    "1. compute the error of the neurons in the hidden layer\n",
    "1. update the hidden layer weights\n",
    "1. Initialize all weights\n",
    "1. compute the activation of each neuron of the hidden layer\n",
    "1. update the output layer weights\n",
    "1. compute the error of the output neuron\n",
    "1. compute the activation of the output layer neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Your solution for task 2 here]**       \n",
    "1. Initialize all weights\n",
    "1. FOR EACH input vector            \n",
    "1. compute the activation of each neuron of the hidden layer         \n",
    "1. REPEAT until stopping criterion is fulfilled\n",
    "1. compute the activation of the output layer neurons\n",
    "1. END REPEAT\n",
    "1. compute the error of the output neuron\n",
    "1. compute the error of the neurons in the hidden layer\n",
    "1. update the output layer weights\n",
    "1. update the hidden layer weights\n",
    "1. END FOR EACH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Linear Activation Function (8 points)\n",
    "It can be shown, if a multilayer perceptron has a linear activation function in all neurons, any number of layers can be reduced to a two-layer input-output model.                                         \n",
    "Consider an MLP with $N$ input neurons $x_n$, one hidden layer with $K$ neurons $z_k$ and a single output $y$. ${\\alpha_k}_n$ define the weights connecting neuron $x_n$ and $z_k$, and $\\beta_k$ the weights connecting $z_k$ and the output. ${\\alpha_k}_0$ and $\\beta_0$ are the weights of the biases. All neurons have a linear activation function, thus the output of a hidden neuron can be written as \n",
    "$$z_k = -{\\alpha_k}_0 + \\sum_{n=1}^{N} {\\alpha_k}_n x_n$$\n",
    "and the total output becomes \n",
    "$$y = -\\beta_0 + \\sum_{k=1}^{K} \\beta_k z_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A) (4 points)** Draw a graph representing the MLP and annotate it with the relevant variables (input, hidden and output neurons, bias and weights)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Your answer for 3A here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B) (4 points)** Show that there exists an MLP without hidden layers, which models the same function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Your answer for 3B here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Programming (15 points)\n",
    "---\n",
    "Like in the first assignment you aim to recognize handwritten digits. This time you will not train a linear perceptron, but a non-linear multilayer perceptron (MLP). You won’t have to implement it – we just want you to play around with existing code and modify it slightly. We are using the ```scikit-learn``` implementation, that can be found here:            \n",
    "http://scikit-learn.org/stable/modules/neural_networks_supervised.html            \n",
    "You might have to install ```scikit-learn``` beforehand. Follow the instructions on their webpage to do so.                   \n",
    "This time we will use the full MNIST Data set.             \n",
    "\n",
    "Below you find the code to load the MNIST dataset and to train an MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import convolve\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "import os.path\n",
    "import scipy as sp\n",
    "import pylab as pl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and loading MNIST data\n",
      "Got MNIST with 52500 training- and 17500 test samples\n",
      "('Digit distribution in whole dataset:', array([6903, 7877, 6990, 7141, 6824, 6313, 6876, 7293, 6825, 6958]))\n",
      "Training model.\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "[CV] hidden_layer_sizes=(256,) .......................................\n",
      "[CV] hidden_layer_sizes=(256,) .......................................\n",
      "[CV] hidden_layer_sizes=(256,) .......................................\n",
      "[CV] hidden_layer_sizes=(256,) .......................................\n",
      "Iteration 1, loss = 0.40642654\n",
      "Iteration 1, loss = 0.41297862\n",
      "Iteration 1, loss = 0.40986989\n",
      "Iteration 1, loss = 0.41142810\n",
      "Iteration 2, loss = 0.17356502\n",
      "Iteration 2, loss = 0.17806639\n",
      "Iteration 2, loss = 0.17954160\n",
      "Iteration 2, loss = 0.18402350\n",
      "Iteration 3, loss = 0.12499738\n",
      "Iteration 3, loss = 0.12821320\n",
      "Iteration 3, loss = 0.12847990\n",
      "Iteration 3, loss = 0.13025889\n",
      "Iteration 4, loss = 0.09665224\n",
      "Iteration 4, loss = 0.09746064\n",
      "Iteration 4, loss = 0.09760662\n",
      "Iteration 4, loss = 0.09959530\n",
      "Iteration 5, loss = 0.07616856\n",
      "Iteration 5, loss = 0.07707669\n",
      "Iteration 5, loss = 0.07757154\n",
      "Iteration 5, loss = 0.08063672\n",
      "Iteration 6, loss = 0.06095472\n",
      "Iteration 6, loss = 0.06290022\n",
      "Iteration 6, loss = 0.06219153\n",
      "Iteration 6, loss = 0.07262333\n",
      "Iteration 7, loss = 0.04953386\n",
      "Iteration 7, loss = 0.05124200\n",
      "Iteration 7, loss = 0.05084863\n",
      "Iteration 7, loss = 0.05363349\n",
      "Iteration 8, loss = 0.04227440\n",
      "Iteration 8, loss = 0.04207273\n",
      "Iteration 8, loss = 0.04210588\n",
      "Iteration 8, loss = 0.04934675\n",
      "Iteration 9, loss = 0.03520068\n",
      "Iteration 9, loss = 0.03438807\n",
      "Iteration 9, loss = 0.03534794\n",
      "Iteration 9, loss = 0.03745059\n",
      "Iteration 10, loss = 0.02938371\n",
      "Iteration 10, loss = 0.02861629\n",
      "Iteration 10, loss = 0.02883173\n",
      "Iteration 10, loss = 0.04834390\n",
      "Iteration 11, loss = 0.02434132\n",
      "Iteration 11, loss = 0.02314685\n",
      "Iteration 11, loss = 0.02380722\n",
      "Iteration 11, loss = 0.02938624\n",
      "Iteration 12, loss = 0.02072375\n",
      "Iteration 12, loss = 0.01952664\n",
      "Iteration 12, loss = 0.01972835\n",
      "Iteration 12, loss = 0.02367493\n",
      "Iteration 13, loss = 0.01647197\n",
      "Iteration 13, loss = 0.01665688\n",
      "Iteration 13, loss = 0.01643719\n",
      "Iteration 13, loss = 0.01992299\n",
      "Iteration 14, loss = 0.01409246\n",
      "Iteration 14, loss = 0.01305357\n",
      "Iteration 14, loss = 0.01321071\n",
      "Iteration 14, loss = 0.01691450\n",
      "Iteration 15, loss = 0.01131503\n",
      "Iteration 15, loss = 0.01166225\n",
      "Iteration 15, loss = 0.01141646\n",
      "Iteration 15, loss = 0.01423672\n",
      "Iteration 16, loss = 0.00983391\n",
      "Iteration 16, loss = 0.00936201\n",
      "Iteration 16, loss = 0.01025026\n",
      "Iteration 16, loss = 0.01252283\n",
      "Iteration 17, loss = 0.00832021\n",
      "Iteration 17, loss = 0.00812617\n",
      "Iteration 17, loss = 0.00853296\n",
      "Iteration 17, loss = 0.01086125\n",
      "Iteration 18, loss = 0.00681632\n",
      "Iteration 18, loss = 0.00671447\n",
      "Iteration 18, loss = 0.00683606\n",
      "Iteration 18, loss = 0.00894226\n",
      "Iteration 19, loss = 0.00582365\n",
      "Iteration 19, loss = 0.00527896\n",
      "Iteration 19, loss = 0.00626078\n",
      "Iteration 19, loss = 0.00754465\n",
      "Iteration 20, loss = 0.00478294\n",
      "Iteration 20, loss = 0.00460630\n",
      "Iteration 20, loss = 0.00505359\n",
      "Iteration 20, loss = 0.00657532\n",
      "Iteration 21, loss = 0.00407049\n",
      "Iteration 21, loss = 0.00387940\n",
      "Iteration 21, loss = 0.00388258\n",
      "Iteration 21, loss = 0.00547839\n",
      "Iteration 22, loss = 0.00364357\n",
      "Iteration 22, loss = 0.00368164\n",
      "Iteration 22, loss = 0.00379062\n",
      "Iteration 22, loss = 0.00483747\n",
      "Iteration 23, loss = 0.00351835\n",
      "Iteration 23, loss = 0.00310228\n",
      "Iteration 23, loss = 0.00344721\n",
      "Iteration 23, loss = 0.00423910\n",
      "Iteration 24, loss = 0.00267633\n",
      "Iteration 24, loss = 0.00265216\n",
      "Iteration 24, loss = 0.00266594\n",
      "Iteration 24, loss = 0.00581129\n",
      "Iteration 25, loss = 0.00236782\n",
      "Iteration 25, loss = 0.00222759\n",
      "Iteration 25, loss = 0.00219813\n",
      "Iteration 25, loss = 0.00365474\n",
      "Iteration 26, loss = 0.00207302\n",
      "Iteration 26, loss = 0.00193137\n",
      "Iteration 26, loss = 0.00253053\n",
      "Iteration 26, loss = 0.00279575\n",
      "Iteration 27, loss = 0.00206110\n",
      "Iteration 27, loss = 0.00173283\n",
      "Iteration 27, loss = 0.00733866\n",
      "Iteration 27, loss = 0.00245645\n",
      "Iteration 28, loss = 0.00184531\n",
      "Iteration 28, loss = 0.00160128\n",
      "Iteration 28, loss = 0.01211094\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV] .. hidden_layer_sizes=(256,), score=0.974002475955, total= 1.7min\n",
      "[CV] hidden_layer_sizes=(256,) .......................................\n",
      "Iteration 28, loss = 0.00212482\n",
      "Iteration 29, loss = 0.00150443\n",
      "Iteration 29, loss = 0.00166336\n",
      "Iteration 29, loss = 0.00191130\n",
      "Iteration 1, loss = 0.40662236\n",
      "Iteration 30, loss = 0.00131327\n",
      "Iteration 30, loss = 0.01654724\n",
      "Iteration 30, loss = 0.00180525\n",
      "Iteration 2, loss = 0.17975291\n",
      "Iteration 31, loss = 0.00125651\n",
      "Iteration 31, loss = 0.01071322\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV] .. hidden_layer_sizes=(256,), score=0.973147971815, total= 1.9min\n",
      "[CV] hidden_layer_sizes=(512,) .......................................\n",
      "Iteration 31, loss = 0.00162151\n",
      "Iteration 3, loss = 0.12845749\n",
      "Iteration 32, loss = 0.00126216\n",
      "Iteration 32, loss = 0.00144959\n",
      "Iteration 4, loss = 0.09925631\n",
      "Iteration 33, loss = 0.00108698\n",
      "Iteration 1, loss = 0.34724715\n",
      "Iteration 33, loss = 0.00144594\n",
      "Iteration 5, loss = 0.07857414\n",
      "Iteration 34, loss = 0.00101810\n",
      "Iteration 34, loss = 0.01706997\n",
      "Iteration 2, loss = 0.15027756\n",
      "Iteration 6, loss = 0.06427160\n",
      "Iteration 35, loss = 0.00097352\n",
      "Iteration 35, loss = 0.01029735\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV] .. hidden_layer_sizes=(256,), score=0.974754691817, total= 2.1min\n",
      "[CV] hidden_layer_sizes=(512,) .......................................\n",
      "Iteration 7, loss = 0.05311619\n",
      "Iteration 36, loss = 0.00092016\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 3, loss = 0.10111429\n",
      "[CV] .. hidden_layer_sizes=(256,), score=0.977437166794, total= 2.2min\n",
      "[CV] hidden_layer_sizes=(512,) .......................................\n",
      "Iteration 8, loss = 0.04381268\n",
      "Iteration 1, loss = 0.35454510\n",
      "Iteration 9, loss = 0.03608966\n",
      "Iteration 4, loss = 0.07389598\n",
      "Iteration 1, loss = 0.35349935\n",
      "Iteration 10, loss = 0.03079947\n",
      "Iteration 2, loss = 0.15034757\n",
      "Iteration 5, loss = 0.05514247\n",
      "Iteration 11, loss = 0.02535322\n",
      "Iteration 2, loss = 0.15574990\n",
      "Iteration 12, loss = 0.02067696\n",
      "Iteration 3, loss = 0.10299378\n",
      "Iteration 6, loss = 0.04265356\n",
      "Iteration 3, loss = 0.10332628\n",
      "Iteration 13, loss = 0.01764968\n",
      "Iteration 4, loss = 0.07434835\n",
      "Iteration 7, loss = 0.03264182\n",
      "Iteration 14, loss = 0.01468211\n",
      "Iteration 4, loss = 0.07368790\n",
      "Iteration 15, loss = 0.01235484\n",
      "Iteration 5, loss = 0.05545883\n",
      "Iteration 8, loss = 0.02513766\n",
      "Iteration 5, loss = 0.05551789\n",
      "Iteration 16, loss = 0.01011532\n",
      "Iteration 6, loss = 0.04358072\n",
      "Iteration 17, loss = 0.00842235\n",
      "Iteration 9, loss = 0.02029197\n",
      "Iteration 6, loss = 0.04226544\n",
      "Iteration 18, loss = 0.00698895\n",
      "Iteration 7, loss = 0.03479454\n",
      "Iteration 10, loss = 0.01566289\n",
      "Iteration 19, loss = 0.00607440\n",
      "Iteration 7, loss = 0.03291847\n",
      "Iteration 20, loss = 0.00496148\n",
      "Iteration 8, loss = 0.02632242\n",
      "Iteration 11, loss = 0.01168040\n",
      "Iteration 8, loss = 0.02565397\n",
      "Iteration 21, loss = 0.00439653\n",
      "Iteration 9, loss = 0.02073164\n",
      "Iteration 22, loss = 0.00364322\n",
      "Iteration 12, loss = 0.00969998\n",
      "Iteration 9, loss = 0.01981962\n",
      "Iteration 23, loss = 0.00343188\n",
      "Iteration 10, loss = 0.01568613\n",
      "Iteration 13, loss = 0.00774585\n",
      "Iteration 10, loss = 0.01551431\n",
      "Iteration 24, loss = 0.01592313\n",
      "Iteration 11, loss = 0.01225561\n",
      "Iteration 25, loss = 0.00376541\n",
      "Iteration 14, loss = 0.00650328\n",
      "Iteration 11, loss = 0.01223195\n",
      "Iteration 26, loss = 0.00230292\n",
      "Iteration 12, loss = 0.00908809\n",
      "Iteration 15, loss = 0.00495556\n",
      "Iteration 27, loss = 0.00268879\n",
      "Iteration 12, loss = 0.00968112\n",
      "Iteration 13, loss = 0.00804134\n",
      "Iteration 28, loss = 0.00184469\n",
      "Iteration 16, loss = 0.00392645\n",
      "Iteration 13, loss = 0.00781218\n",
      "Iteration 29, loss = 0.00165857\n",
      "Iteration 14, loss = 0.00606390\n",
      "Iteration 17, loss = 0.00364664\n",
      "Iteration 30, loss = 0.00148262\n",
      "Iteration 14, loss = 0.00638063\n",
      "Iteration 15, loss = 0.00481444\n",
      "Iteration 31, loss = 0.00137350\n",
      "Iteration 18, loss = 0.00300571\n",
      "Iteration 15, loss = 0.00506211\n",
      "Iteration 32, loss = 0.00126841\n",
      "Iteration 16, loss = 0.00381901\n",
      "Iteration 33, loss = 0.00119188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19, loss = 0.00234996\n",
      "Iteration 16, loss = 0.00406724\n",
      "Iteration 34, loss = 0.00118447\n",
      "Iteration 17, loss = 0.00320654\n",
      "Iteration 20, loss = 0.00194831\n",
      "Iteration 17, loss = 0.00302918\n",
      "Iteration 35, loss = 0.01925881\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV] .. hidden_layer_sizes=(256,), score=0.972370426829, total= 2.4min\n",
      "[CV] hidden_layer_sizes=(512,) .......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  4.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, loss = 0.00264193\n",
      "Iteration 21, loss = 0.00164648\n",
      "Iteration 18, loss = 0.00275922\n",
      "Iteration 19, loss = 0.00224224\n",
      "Iteration 1, loss = 0.36705543\n",
      "Iteration 22, loss = 0.00157695\n",
      "Iteration 19, loss = 0.00284498\n",
      "Iteration 20, loss = 0.00202360\n",
      "Iteration 2, loss = 0.15619505\n",
      "Iteration 23, loss = 0.02113121\n",
      "Iteration 20, loss = 0.00224578\n",
      "Iteration 21, loss = 0.00178836\n",
      "Iteration 3, loss = 0.10459253\n",
      "Iteration 24, loss = 0.01315171\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 21, loss = 0.00179684\n",
      "[CV] .. hidden_layer_sizes=(512,), score=0.974676313785, total= 2.7min\n",
      "[CV] hidden_layer_sizes=(512,) .......................................\n",
      "Iteration 22, loss = 0.00151342\n",
      "Iteration 4, loss = 0.08346322\n",
      "Iteration 22, loss = 0.00162237\n",
      "Iteration 1, loss = 0.34216352\n",
      "Iteration 23, loss = 0.00139090\n",
      "Iteration 5, loss = 0.05841853\n",
      "Iteration 23, loss = 0.00135531\n",
      "Iteration 24, loss = 0.00129817\n",
      "Iteration 6, loss = 0.04708451\n",
      "Iteration 2, loss = 0.14939431\n",
      "Iteration 24, loss = 0.00125249\n",
      "Iteration 25, loss = 0.00121412\n",
      "Iteration 7, loss = 0.03449204\n",
      "Iteration 3, loss = 0.10332593\n",
      "Iteration 25, loss = 0.00120847\n",
      "Iteration 26, loss = 0.00111333\n",
      "Iteration 8, loss = 0.02757168\n",
      "Iteration 4, loss = 0.07424403\n",
      "Iteration 26, loss = 0.00108277\n",
      "Iteration 27, loss = 0.00101524\n",
      "Iteration 9, loss = 0.02237036\n",
      "Iteration 5, loss = 0.05646219\n",
      "Iteration 27, loss = 0.00103988\n",
      "Iteration 28, loss = 0.00095293\n",
      "Iteration 10, loss = 0.01746047\n",
      "Iteration 6, loss = 0.04286257\n",
      "Iteration 28, loss = 0.00093818\n",
      "Iteration 29, loss = 0.00088636\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 11, loss = 0.02882616\n",
      "[CV] .. hidden_layer_sizes=(512,), score=0.977147210055, total= 3.2min\n",
      "[CV] hidden_layer_sizes=(128, 256, 128) ..............................\n",
      "Iteration 7, loss = 0.03382778\n",
      "Iteration 29, loss = 0.00091195\n",
      "Iteration 12, loss = 0.01200470\n",
      "Iteration 1, loss = 0.39059621\n",
      "Iteration 8, loss = 0.03322899\n",
      "Iteration 2, loss = 0.14200577\n",
      "Iteration 30, loss = 0.00084504\n",
      "Iteration 13, loss = 0.00883626\n",
      "Iteration 9, loss = 0.02078618\n",
      "Iteration 3, loss = 0.09811386\n",
      "Iteration 31, loss = 0.00082179\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 14, loss = 0.01335569\n",
      "[CV] .. hidden_layer_sizes=(512,), score=0.978573469193, total= 3.5min\n",
      "[CV] hidden_layer_sizes=(128, 256, 128) ..............................\n",
      "Iteration 10, loss = 0.01684893\n",
      "Iteration 4, loss = 0.07151447\n",
      "Iteration 15, loss = 0.00615423\n",
      "Iteration 1, loss = 0.41264937\n",
      "Iteration 5, loss = 0.05906643\n",
      "Iteration 11, loss = 0.02369335\n",
      "Iteration 2, loss = 0.15000603\n",
      "Iteration 6, loss = 0.04290971\n",
      "Iteration 16, loss = 0.00457140\n",
      "Iteration 12, loss = 0.01078663\n",
      "Iteration 3, loss = 0.10053231\n",
      "Iteration 7, loss = 0.03424871\n",
      "Iteration 17, loss = 0.00366086\n",
      "Iteration 13, loss = 0.00929607\n",
      "Iteration 4, loss = 0.07377859\n",
      "Iteration 8, loss = 0.02757220\n",
      "Iteration 18, loss = 0.00304142\n",
      "Iteration 5, loss = 0.05852588\n",
      "Iteration 9, loss = 0.02022270\n",
      "Iteration 14, loss = 0.00626694\n",
      "Iteration 6, loss = 0.04657060\n",
      "Iteration 19, loss = 0.00398379\n",
      "Iteration 10, loss = 0.01509000\n",
      "Iteration 15, loss = 0.00525640\n",
      "Iteration 7, loss = 0.03625772\n",
      "Iteration 11, loss = 0.01676940\n",
      "Iteration 20, loss = 0.00239165\n",
      "Iteration 8, loss = 0.02846219\n",
      "Iteration 16, loss = 0.00424404\n",
      "Iteration 12, loss = 0.01673617\n",
      "Iteration 21, loss = 0.00213825\n",
      "Iteration 9, loss = 0.02155797\n",
      "Iteration 13, loss = 0.01027961\n",
      "Iteration 17, loss = 0.00367247\n",
      "Iteration 22, loss = 0.00183773\n",
      "Iteration 10, loss = 0.01906713\n",
      "Iteration 14, loss = 0.01570562\n",
      "Iteration 18, loss = 0.00299511\n",
      "Iteration 11, loss = 0.01665088\n",
      "Iteration 15, loss = 0.01191331\n",
      "Iteration 23, loss = 0.00262162\n",
      "Iteration 12, loss = 0.02015965\n",
      "Iteration 19, loss = 0.00268057\n",
      "Iteration 16, loss = 0.00961349\n",
      "Iteration 24, loss = 0.00158440\n",
      "Iteration 13, loss = 0.01661488\n",
      "Iteration 17, loss = 0.01258926\n",
      "Iteration 20, loss = 0.00235272\n",
      "Iteration 25, loss = 0.00138220\n",
      "Iteration 14, loss = 0.01030687\n",
      "Iteration 18, loss = 0.00850605\n",
      "Iteration 21, loss = 0.00213143\n",
      "Iteration 26, loss = 0.00124028\n",
      "Iteration 15, loss = 0.01063808\n",
      "Iteration 19, loss = 0.00676119\n",
      "Iteration 22, loss = 0.00185484\n",
      "Iteration 16, loss = 0.01069087\n",
      "Iteration 20, loss = 0.01570223\n",
      "Iteration 27, loss = 0.00116217\n",
      "Iteration 17, loss = 0.00955171\n",
      "Iteration 21, loss = 0.00850952\n",
      "Iteration 23, loss = 0.00165207\n",
      "Iteration 28, loss = 0.00110860\n",
      "Iteration 18, loss = 0.00836609\n",
      "Iteration 22, loss = 0.00686012\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  hidden_layer_sizes=(128, 256, 128), score=0.974581111957, total= 1.7min\n",
      "[CV] hidden_layer_sizes=(128, 256, 128) ..............................\n",
      "Iteration 24, loss = 0.00141515\n",
      "Iteration 29, loss = 0.00102096\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 19, loss = 0.01245799\n",
      "[CV] ... hidden_layer_sizes=(512,), score=0.98094693722, total= 3.0min\n",
      "[CV] hidden_layer_sizes=(128, 256, 128) ..............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  15 | elapsed:  7.2min remaining:  3.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.39722883\n",
      "Iteration 25, loss = 0.00137260\n",
      "Iteration 20, loss = 0.01169141\n",
      "Iteration 1, loss = 0.40835696\n",
      "Iteration 2, loss = 0.14947995\n",
      "Iteration 21, loss = 0.00922874\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  hidden_layer_sizes=(128, 256, 128), score=0.97438583127, total= 1.6min\n",
      "[CV] hidden_layer_sizes=(128, 256, 128) ..............................\n",
      "Iteration 26, loss = 0.00122344\n",
      "Iteration 2, loss = 0.17099341\n",
      "Iteration 3, loss = 0.09737475\n",
      "Iteration 3, loss = 0.11231319\n",
      "Iteration 27, loss = 0.00114278\n",
      "Iteration 4, loss = 0.07164677\n",
      "Iteration 1, loss = 0.39376984\n",
      "Iteration 4, loss = 0.07961975\n",
      "Iteration 5, loss = 0.05052172\n",
      "Iteration 2, loss = 0.13824705\n",
      "Iteration 28, loss = 0.00105494\n",
      "Iteration 5, loss = 0.05970548\n",
      "Iteration 6, loss = 0.04095516\n",
      "Iteration 3, loss = 0.09740214\n",
      "Iteration 29, loss = 0.03345718\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV] ...... hidden_layer_sizes=(512,), score=0.97265625, total= 3.0min\n",
      "Iteration 6, loss = 0.04665250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  15 | elapsed:  7.6min remaining:  1.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.03437354\n",
      "Iteration 4, loss = 0.07072031\n",
      "Iteration 7, loss = 0.03669869\n",
      "Iteration 8, loss = 0.02477591\n",
      "Iteration 5, loss = 0.05275092\n",
      "Iteration 8, loss = 0.02755860\n",
      "Iteration 9, loss = 0.01802605\n",
      "Iteration 6, loss = 0.05792011\n",
      "Iteration 9, loss = 0.02321964\n",
      "Iteration 10, loss = 0.01453453\n",
      "Iteration 7, loss = 0.03344756\n",
      "Iteration 10, loss = 0.01786765\n",
      "Iteration 11, loss = 0.01567504\n",
      "Iteration 8, loss = 0.04729200\n",
      "Iteration 11, loss = 0.01681748\n",
      "Iteration 12, loss = 0.01227599\n",
      "Iteration 9, loss = 0.02190519\n",
      "Iteration 12, loss = 0.01444114\n",
      "Iteration 13, loss = 0.01290528\n",
      "Iteration 10, loss = 0.01777878\n",
      "Iteration 13, loss = 0.01830027\n",
      "Iteration 14, loss = 0.01395799\n",
      "Iteration 11, loss = 0.01296514\n",
      "Iteration 14, loss = 0.01147425\n",
      "Iteration 15, loss = 0.00978983\n",
      "Iteration 12, loss = 0.01265858\n",
      "Iteration 15, loss = 0.00940699\n",
      "Iteration 16, loss = 0.01315197\n",
      "Iteration 13, loss = 0.00910297\n",
      "Iteration 16, loss = 0.00777036\n",
      "Iteration 17, loss = 0.00903016\n",
      "Iteration 14, loss = 0.01175246\n",
      "Iteration 17, loss = 0.01322969\n",
      "Iteration 18, loss = 0.00926536\n",
      "Iteration 15, loss = 0.01284224\n",
      "Iteration 18, loss = 0.01066302\n",
      "Iteration 19, loss = 0.01272097\n",
      "Iteration 16, loss = 0.01433986\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  hidden_layer_sizes=(128, 256, 128), score=0.973608993902, total=  50.2s\n",
      "Iteration 19, loss = 0.02316895\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 20, loss = 0.00894200\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  hidden_layer_sizes=(128, 256, 128), score=0.973802038678, total= 1.0min\n",
      "[CV]  hidden_layer_sizes=(128, 256, 128), score=0.971812208361, total= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  8.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.31480014\n",
      "Iteration 2, loss = 0.13265725\n",
      "Iteration 3, loss = 0.08846696\n",
      "Iteration 4, loss = 0.06458865\n",
      "Iteration 5, loss = 0.04799160\n",
      "Iteration 6, loss = 0.03642691\n",
      "Iteration 7, loss = 0.02812341\n",
      "Iteration 8, loss = 0.02251528\n",
      "Iteration 9, loss = 0.01718398\n",
      "Iteration 10, loss = 0.01316793\n",
      "Iteration 11, loss = 0.01070230\n",
      "Iteration 12, loss = 0.00836069\n",
      "Iteration 13, loss = 0.00677829\n",
      "Iteration 14, loss = 0.00476518\n",
      "Iteration 15, loss = 0.00403162\n",
      "Iteration 16, loss = 0.00353455\n",
      "Iteration 17, loss = 0.00276550\n",
      "Iteration 18, loss = 0.00275566\n",
      "Iteration 19, loss = 0.00262856\n",
      "Iteration 20, loss = 0.00176854\n",
      "Iteration 21, loss = 0.00141403\n",
      "Iteration 22, loss = 0.00120354\n",
      "Iteration 23, loss = 0.00108292\n",
      "Iteration 24, loss = 0.00102183\n",
      "Iteration 25, loss = 0.00096207\n",
      "Iteration 26, loss = 0.00090132\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "('Finished with grid search with best mean cross-validated score:', 0.9768)\n",
      "('Best params appeared to be', {'hidden_layer_sizes': (512,)})\n"
     ]
    }
   ],
   "source": [
    "PATH = 'mlp_model.pkl'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('Fetching and loading MNIST data')\n",
    "    #loads mnist dataset\n",
    "    mnist = fetch_mldata('MNIST original')\n",
    "    # separate dataset into two arrays X- training samples represented as floating point feature vectors\n",
    "    #                                  y- target values for the training samples\n",
    "    X, y = mnist.data, mnist.target\n",
    "    # split X and y into random train (75% of data set) and test subsets( 25% ) randomly with seed used \n",
    "    # by the random number generator\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X / 255., y, test_size=0.25, random_state=0)\n",
    "\n",
    "    print('Got MNIST with %d training- and %d test samples' % (len(y_train), len(y_test)))\n",
    "    print('Digit distribution in whole dataset:', np.bincount(y.astype('int64')))\n",
    "\n",
    "    clf = None\n",
    "    # Load the model from file if there is some\n",
    "    if os.path.exists(PATH):\n",
    "        print('Loading model from file.')\n",
    "        clf = joblib.load(PATH).best_estimator_\n",
    "    # else train model\n",
    "    else:\n",
    "        print('Training model.')\n",
    "        # Create 3 different size for hidden layers and its number of neurons: \n",
    "        # one hidden layer with 256 neurons\n",
    "        # one hidden layer with 512 neurons\n",
    "        # and three hiddden layers with 128 in the first and third layer and 256 in the second\n",
    "        params = {'hidden_layer_sizes': [(256,), (512,), (128, 256, 128,)]}\n",
    "        # create implemented scikit-learn MLP Classifier that will print its progress and adaptive learning rate \n",
    "        # which keeps the learning rate constant to 0.001 as long as training loss keeps decreasing.\n",
    "        mlp = MLPClassifier(verbose=10, learning_rate='adaptive')\n",
    "        # Exhaustive Grid Search exhaustively generates candidates from a grid of parameter values specified \n",
    "        # in variable params to find the reasonable estimator, priting its probgress, not running in pararel and \n",
    "        # using the 5-fold cross-validation strategy\n",
    "        clf = GridSearchCV(mlp, params, verbose=10, n_jobs=-1, cv=5)\n",
    "        #Learns from training set\n",
    "        clf.fit(X_train, y_train)\n",
    "        print('Finished with grid search with best mean cross-validated score:', clf.best_score_)\n",
    "        print('Best params appeared to be', clf.best_params_)\n",
    "        # dump clf to file\n",
    "        joblib.dump(clf, PATH)\n",
    "        # sets the estimator which gave the highest score as classifier\n",
    "        clf = clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A) (4 points)** Shortly explain in your own words, what the code does. Ideally explain it line-by-line (```print``` statements can be omitted). You can write short comments directly in the code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B) (1 point)**  Run the code (this may take a while when running it for the first time). What are the training and testing errors?         \n",
    "*Hint: The current progress is printed on the Jupyter Notebook terminal.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9822285714285715\n"
     ]
    }
   ],
   "source": [
    "print(clf.score(X_train, y_train))\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training error is **~[0.0]**.                      \n",
    "The test error is **~[0.01777142857]**.             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C) (4 points)** What does ```GridSearchCV``` do? Do we really need this function? Explain your decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Your answer for C here]**         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D) (3 points)** What role plays the ```random_state``` parameter in ```train_test_split```? What happens if we left it out?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Your answer for D here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E) (3 points)** We now want to compare an MLP without any hidden units with a single Perceptron. To do so, first train an MLP without an hidden layer by changing the given code. Print its training and test error. Compare the result to the one you obtained when training the Perceptron on the ```USPS``` dataset (assignment 2). Is this MLP then the same algorithm as the Perceptron of assignment 2? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code for task E) here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training error is **~[training error]**.                   \n",
    "The test error is **~[test error]**.              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Your answer for E here]**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
