{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cognitive Algorithms - Assignment 4 (30 points)\n",
    "Cognitive Algorithms        \n",
    "Summer term 2018      \n",
    "Technische Universit√§t Berlin     \n",
    "Fachgebiet Maschinelles Lernen \n",
    "\n",
    "**Due on June 20, 2018 10am via ISIS **\n",
    "                    \n",
    "After completing all tasks, run the whole notebook so that the content of each cell is properly displayed. Make sure that the code was ran and the entire output (e.g. figures) is printed. Print the notebook as a PDF file and again make sure that all lines are readable - use line breaks in the Python Code '\\' if necessary. Points will be deducted, if code or content is not readable!                  \n",
    "           \n",
    "**Upload the PDF file that contains a copy of your notebook on ISIS.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group:        \n",
    "Members:          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Theory (8 points)\n",
    "---\n",
    "Let $\\varphi$ be a function, that maps the input data $x_1, \\ldots, x_n \\in \\mathbb{R}^d$ to some finite or infinite dimensional $\\mathbb{R}$-vector space (so-called *feature space*). The *representer theorem* states that if a function $k: \\mathbb{R}^d \\times \\mathbb{R}^d \\to \\mathbb R$ is a *valid kernel*, then it defines the scalar product of the input data in that feature space\n",
    "$$ k(x, x') \\; = \\; \\langle \\varphi(x), \\varphi(x') \\rangle \\; \\; \\; \\; \\; \\text{for all} \\; \\;  x, x' \\in \\mathbb{R}^d$$\n",
    "The function $k$ is a *valid kernel*, iff it satisfies the *Mercer's condition*, which verifies that for any input data $x_1, \\ldots, x_n \\in \\mathbb{R}^d$ and coefficients $c_1, \\ldots, c_n \\in \\mathbb{R}$ the inequality \n",
    "$$ \\sum_{i=1}^{n} \\sum_{j=1}^n c_i c_j k(x_i, x_j) \\geq 0 $$\n",
    "is satisfied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A) (4 points)** Show that the sum of two valid kernels $k_1$ and $k_2$ is again a valid kernel, i.e. that \n",
    "$$k(x,x') := k_1(x, x') + k_2(x,x')$$\n",
    "satisfies the Mercer's condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Your derivation for A) here]**\n",
    "$$ \\sum_{i=1}^{n} \\sum_{j=1}^n c_i c_j k(x_i, x_j)$$\n",
    "$$ k(x,x') := k_1(x, x') + k_2(x,x') $$\n",
    "$$ \\sum_{i=1}^{n} \\sum_{j=1}^n c_i c_j (k_1(x_i, x_j) + k_2(x_i,x_j)) $$\n",
    "$$ \\sum_{i=1}^{n} \\sum_{j=1}^n c_i c_j k_1(x_i, x_j) + \\sum_{i=1}^{n} \\sum_{j=1}^n c_i c_j k_2(x_i,x_j)$$\n",
    "\n",
    "Both $ k_1 $ and $ k_2 $ are valid kernels: \n",
    "$$ \\sum_{i=1}^{n} \\sum_{j=1}^n c_i c_j k_1(x_i, x_j) \\geq 0 $$ \n",
    "and \n",
    "$$ \\sum_{i=1}^{n} \\sum_{j=1}^n c_i c_j k_2(x_i,x_j) \\geq 0 $$ \n",
    "\n",
    "That means that the sum is also $ \\geq 0 $ and hence:\n",
    "$$ \\sum_{i=1}^{n} \\sum_{j=1}^n c_i c_j k_1(x_i, x_j) + c_i c_j k_2(x_i,x_j) \\geq 0 $$\n",
    "which satisfies the *Mercer's condition*,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B) (4 point)** Let $\\varphi_1: \\mathbb{R}^d \\to \\mathbb{R}^{h_1}$ and $\\varphi_2: \\mathbb{R}^d \\to \\mathbb{R}^{h_2}$ the feature mappings of $k_1$ and $k_2$. Give the feature mapping for the kernel $k$, i.e. a mapping $\\varphi$ such that \n",
    "$$k(x, x') \\; = \\; k_1(x, x') + k_2(x,x') \\; = \\; \\langle \\varphi(x), \\varphi(x') \\rangle$$\n",
    "and show that it fulfills the representer theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Your derivation for B) here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Programming (22 points)\n",
    "---\n",
    "\n",
    "The application in this assignment is the same as in assignment 4. You will predict two dimensional hand positions $y \\in \\mathbb{R}^2$ from electromyographic (EMG) recordings $x \\in \\mathbb{R}^{192}$ obtained with high-density electrode arrays on the lower arm.     \n",
    "Labels are 2D positions of the hand during different hand movements.\n",
    "\n",
    "Remember that even after 'linearizing' the EMG-hand position relationship by computing the log of the EMG features, the relationship is not exactly linear. Also we do not know the exact non-linearity; it might not be the same for all regions in EMG space and for all electrodes. So we can hope to gain something from using a non-parametric and non-linear method like kernel ridge regression.\n",
    "\n",
    "The criterion to evaluate the model and select optimal parameters is the so called coefficient of determination, or $r^2$ index\n",
    "$$ r^2 = 1-\\frac{\\sum_{d=1}^D \\mathbb{V}(\\hat{y}_d-y_d)}{\\sum_{d=1}^D \\mathbb{V}(y_d)} $$\n",
    "where $D$ is the dimensionality of the data labels, $y$ are the true labels and $\\hat{y}$ the estimated labels. This score is 1 for perfect predictions and smaller otherwise.\n",
    "\n",
    "Use the data set ```myo_data.mat``` from the last assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "import scipy as sp\n",
    "from numpy.linalg import inv\n",
    "from numpy.linalg import solve\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(fname):\n",
    "    ''' Loads EMG data from <fname> '''\n",
    "    # load the data\n",
    "    data = loadmat(fname)\n",
    "    # extract data for training\n",
    "    X_train = data['training_data']\n",
    "    X_train = sp.log(X_train)\n",
    "    X_train = X_train[:, :1000]\n",
    "    # extract hand positions\n",
    "    Y_train = data['training_labels']\n",
    "    Y_train = Y_train[:, :1000]\n",
    "    return X_train,Y_train\n",
    "\n",
    "def GaussianKernel(X1, X2, kwidth):\n",
    "    ''' Compute Gaussian Kernel \n",
    "    Input: X1    - DxN1 array of N1 data points with D features \n",
    "           X2    - DxN2 array of N2 data points with D features \n",
    "           kwidth - Kernel width\n",
    "    Output K     - N1 x N2 Kernel matrix\n",
    "    '''\n",
    "    assert(X1.shape[0] == X2.shape[0])\n",
    "    K = cdist(X1.T, X2.T, 'euclidean')\n",
    "    K = np.exp(-(K ** 2) / (2. * kwidth ** 2))\n",
    "    return K\n",
    "\n",
    "def train_krr(X_train, Y_train,kwidth,llambda):\n",
    "    ''' Trains kernel ridge regression (krr)\n",
    "    Input:       X_train  -  DxN array of N data points with D features\n",
    "                 Y        -  D2xN array of length N with D2 multiple labels\n",
    "                 kwdith   -  kernel width\n",
    "                 llambda    -  regularization parameter\n",
    "    Output:      alphas   -  NxD2 array, weighting of training data used for apply_krr                     \n",
    "    '''\n",
    "    # your code here\n",
    "    \n",
    "def apply_krr(alphas, X_train, X_test, kwidth):\n",
    "    ''' Applys kernel ridge regression (krr)\n",
    "    Input:      alphas      -  NtrxD2 array trained in train_krr      \n",
    "                X_train     -  DxNtr array of Ntr train data points with D features\n",
    "                X_test      -  DxNte array of Nte test data points with D features\n",
    "                kwidht      -  Kernel width             \n",
    "    Output:     Y_test      -  D2xNte array\n",
    "    '''\n",
    "    # your code here\n",
    "\n",
    "def train_ols(X_train, Y_train):\n",
    "    ''' Trains ordinary least squares (ols) regression \n",
    "    Input:       X_train  -  DxN array of N data points with D features\n",
    "                 Y        -  D2xN array of length N with D2 multiple labels\n",
    "    Output:      W        -  DxD2 array, linear mapping used to estimate labels \n",
    "                             with sp.dot(W.T, X)                      \n",
    "    '''\n",
    "    #W = sp.dot(inv(sp.dot(X_train,X_train.T)),sp.dot(X_train,Y_train.T))\n",
    "    W = solve(sp.dot(X_train,X_train.T),sp.dot(X_train,Y_train.T))\n",
    "    return W\n",
    "    \n",
    "def apply_ols(W, X_test):\n",
    "    ''' Applys ordinary least squares (ols) regression \n",
    "    Input:       X_test    -  DxN array of N data points with D features\n",
    "                 W        -  DxD2 array, linear mapping used to estimate labels \n",
    "                             trained with train_ols                   \n",
    "    Output:     Y_test    -  D2xN array\n",
    "    '''\n",
    "    Y_test = sp.dot(W.T, X_test)\n",
    "    return Y_test\n",
    "\n",
    "def test_handpositions():\n",
    "    X,Y = load_data('myo_data.mat')\n",
    "    crossvalidate_krr(X,Y)\n",
    "\n",
    "def test_sine_toydata(kwidth = 1, llambda = 1):\n",
    "    #Data generation\n",
    "    X_train = sp.arange(0,10,.01)\n",
    "    X_train = X_train[None,:]\n",
    "    Y_train = sp.sin(X_train) + sp.random.normal(0,.5,X_train.shape)\n",
    "    #Linear Regression \n",
    "    w_est = train_ols(X_train, Y_train) \n",
    "    Y_est_lin = apply_ols(w_est,X_train)\n",
    "    #Kernel Ridge Regression\n",
    "    alphas = train_krr(X_train,Y_train,kwidth,llambda)\n",
    "    Y_est_krr = apply_krr(alphas,X_train,X_train,kwidth)\n",
    "    \n",
    "    #Plot result\n",
    "    pl.figure()\n",
    "    pl.plot(X_train.T, Y_train.T, '+k', label = 'Train Data')\n",
    "    pl.plot(X_train.T, Y_est_lin.T, '-.', linewidth = 2, label = 'OLS')\n",
    "    pl.plot(X_train.T, Y_est_krr.T,  'r', linewidth = 2, label = 'KRR')\n",
    "    pl.xlabel('x')\n",
    "    pl.ylabel('y')\n",
    "    pl.title(r'$\\lambda$ = ' + str(llambda) + '   $\\sigma$ = ' + str(kwidth))\n",
    "    pl.legend(loc = 'lower right')\n",
    "    \n",
    "      \n",
    "def crossvalidate_krr(X,Y,f=5, kwidths=10.0**np.array([0, 1, 2]), llambdas=10.0**np.array([-4, -2, 0])):\n",
    "    ''' \n",
    "    Test generalization performance of kernel ridge regression with gaussian kernel\n",
    "    Input:      X   data (dims-by-samples)\n",
    "                Y   labels (dims2-by-samples)\n",
    "                f   number of cross-validation folds\n",
    "                kwidths width of gaussian kernel function \n",
    "                llambdas regularizer (height of ridge on kernel matrix)\n",
    "    '''\n",
    "    N = f*(X.shape[-1]/f)\n",
    "    idx = sp.reshape(sp.random.permutation(sp.arange(N)),(f,N/f))\n",
    "    r2_outer = sp.zeros((f))\n",
    "    r2_linear = sp.zeros((f))\n",
    "    r2_inner = sp.zeros((f-1,kwidths.shape[-1],llambdas.shape[-1]))\n",
    "    \n",
    "    # to outer cross-validation (model evaluation)\n",
    "    for ofold in range(f):\n",
    "        # split in training and test (outer fold)\n",
    "        otestidx = sp.zeros((f),dtype=bool)\n",
    "        otestidx[ofold] = 1\n",
    "        otest = idx[otestidx,:].flatten()\n",
    "        otrain = idx[~otestidx,:]\n",
    "        \n",
    "        # inner cross-validation (model selection)\n",
    "        for ifold in range(f-1):\n",
    "            # split in training and test (inner fold)\n",
    "            itestidx = sp.zeros((f-1),dtype=bool)\n",
    "            itestidx[ifold] = 1\n",
    "            itest = otrain[itestidx,:].flatten()\n",
    "            itrain = otrain[~itestidx,:].flatten()\n",
    "            \n",
    "            # do inner cross-validation (model selection)\n",
    "            for illambda in range(llambdas.shape[-1]):\n",
    "                for ikwidth in range(kwidths.shape[-1]):\n",
    "                    #compute kernel for all data points\n",
    "                    alphas = train_krr(X[:,itrain],Y[:,itrain],kwidths[ikwidth],llambdas[illambda])\n",
    "                    yhat = apply_krr(alphas, X[:,itrain], X[:,itest],kwidths[ikwidth])\n",
    "                    r2_inner[ifold,ikwidth,illambda] = compute_rsquare(yhat,Y[:,itest])\n",
    "\n",
    "        #train again using optimal parameters\n",
    "        r2_across_folds = r2_inner.mean(axis=0)\n",
    "        optkwidthidx, optllambdaidx = np.unravel_index(r2_across_folds.flatten().argmax(),r2_across_folds.shape)\n",
    "        #evaluate model on outer test fold\n",
    "        alphas = train_krr(X[:,otrain.flatten()],Y[:,otrain.flatten()], kwidths[optkwidthidx],llambdas[optllambdaidx])\n",
    "        yhat = apply_krr(alphas, X[:,otrain.flatten()],X[:,otest], kwidths[optkwidthidx])\n",
    "        r2_outer[ofold] = compute_rsquare(yhat,Y[:,otest])\n",
    "\n",
    "        # for comparison: predict with linear model\n",
    "        w_est = train_ols(X[:,otrain.flatten()], Y[:,otrain.flatten()]) \n",
    "        y_est_lin = apply_ols(w_est,X[:,otest])\n",
    "        r2_linear[ofold] = compute_rsquare(y_est_lin,Y[:,otest])\n",
    "        \n",
    "        print 'Fold %d'%ofold + ' best kernel width %f'%kwidths[optkwidthidx] +\\\n",
    "        ' best regularizer %f'%llambdas[optllambdaidx] + \\\n",
    "        ' rsquare %f'%r2_outer[ofold] + \\\n",
    "        ' rsquare linear %f'%r2_linear[ofold]\n",
    "    pl.figure()\n",
    "    pl.boxplot(sp.vstack((r2_outer,r2_linear)).T)\n",
    "    pl.ylabel('$r^2$')\n",
    "    pl.xticks((1,2),('KRR','Lin'))\n",
    "    #pl.savefig('krr_vs_lin_comparison.pdf')\n",
    "    return r2_outer,r2_linear\n",
    "    \n",
    "def compute_rsquare(yhat,Y):\n",
    "    '''compute coefficient of determination'''\n",
    "    return 1 - (sp.var((yhat - Y),axis=1).sum()/sp.var(Y,axis=1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A) (6 points)** Implement Kernel Ridge Regression (KRR) by completing the function stubs  ```krr_train``` and  ```krr_apply```. \n",
    "We use the notation from assignment 4, \n",
    "$$ X_{\\text{train}} \\in \\mathbb{R}^{D_X \\times N_{tr}}, \\;  Y_{\\text{train}} \\in \\mathbb{R}^{D_Y \\times N_{tr}}, \\; X_{\\text{test}} \\in \\mathbb{R}^{D_X \\times N_{te}} $$\n",
    "In ```krr_train```, you estimate a linear combination of the input vectors $\\alpha$, \n",
    "$$\\alpha = (K + \\lambda I)^{-1}Y_{\\text{train}}^T$$\n",
    "where $\\lambda$ is the regularization parameter and $K$ is the $N_{tr} \\times N_{tr}$ Gaussian Kernel matrix with Kernel width $\\sigma$, $K_{ij} =  \\exp\\left( - \\frac{\\| X_{\\text{train}}^i - X_{\\text{train}}^j\\|^2}{\\sigma^2} \\right)$. You can compute $K$ with the provided function ```GaussianKernel```.\n",
    "\n",
    "The function ```krr_apply``` than uses the weights $\\alpha$ to predict the (unknown) hand positions of new test data $X_{\\text{test}}$\n",
    "$$ Y_{\\text{test}} = (\\mathbf{k} \\alpha)^T.$$\n",
    "where $\\mathbf k$ is the $N_{\\text{test}} \\times N_{\\text{train}}$ matrix $\\mathbf{k}_{ij} =   \\exp\\left( - \\frac{\\| X_{\\text{test}}^i - X_{\\text{train}}^j\\|^2}{\\sigma^2}\\right) $.    \n",
    "\n",
    "The function  ```test_sine_toydata``` helps you to debug your code. It generates toy data that follows a sine function, $x_i \\in \\{0, 0.01, 0.02, \\ldots, 10\\}, y_i = \\sin(x_i) + \\epsilon, \\epsilon \\sim \\mathcal{N}(0, 0.5)$. The result of KRR should resemble the sine function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sine_toydata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B) (5 points)** We want to analyse how the Kernel Ridge solution depends on its hyperparameters, the kernel width $\\sigma$ and  the regularization parameter $\\lambda$. \n",
    "- Call the function  ```test_sine_toydata```  with $\\lambda$ = 1 for three different Kernel widths, $\\sigma = \\{ 0.1, 1, 10\\}$. How does the Kernel width affect the solution? Explain the observed behaviour.  \n",
    "- Call the function  ```test_sine_toydata```  with $\\sigma$ = 1 for three different regularization parameters, $\\lambda = \\{ 10^{-10}, 1, 500\\}$. How does the regularization parameter affect the solution? Explain the observed behaviour.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Your answers for B) here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sine_toydata(kwidth = 0.1, llambda = 1)\n",
    "test_sine_toydata(kwidth = 1, llambda = 1)\n",
    "test_sine_toydata(kwidth = 10, llambda = 1)\n",
    "test_sine_toydata(kwidth = 1, llambda = 1e-10)\n",
    "test_sine_toydata(kwidth = 1, llambda = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C) (4 points)**  Briefly explain in your own words how nested-crossvalidation is done. To do so, you can examine the function ```crossvalidate_krr```.Explain briefly how $\\lambda$ and $\\sigma$ are chosen within the function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Your answers for C) here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D) (6 points)** Predict two dimensional hand positions with Kernel Ridge Regression by calling the function ``` test_handpositions```. It shows a boxplot for the linear regression and the Kernel Ridge Regression.  What does a boxplot show? (check the help function in python or the wikipedia article). Do we gain something from Kernel Ridge Regression as compared to simple linear regression? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Your answers for D) here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_handpositions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E) (1 point)** In the last task, we have applied the function ```test_handpositions``` only to the first $1000$ data points out of the $10255$ available data points. Why did we do so in this exercice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Your answers for E) here]**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
